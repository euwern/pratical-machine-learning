---
title: "Practical Machine Learning"
author: "euwern"
date: "Saturday, May 23, 2015"
output: html_document
---

In this course project our goal is to predict "classe" variable of the given test data by learning the training data. 
The "classe" variable consists of 5 values (A to E) and it use to classify how the participant do the dumbbell biceps curl. You can read more about the experiment in http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises

Random Forest Machine learning algorithm is used to predict the "classe" variable. 

Preparing the data
-------------------

```{r,warning=FALSE}
library(caret)
train_dat <- read.csv(file="pml-training.csv")

inTrain <- createDataPartition(y=train_dat$classe, p=0.7, list=F)

training <- train_dat[inTrain,]
crossVal <- train_dat[-inTrain,]
```

Data Pre-processing and Co-variate creation
---------------------------------------------
The raw data consists of 160 variables but not all variables are useful in making the prediction. If you observe the training data, you can see a lot of NA values. We also want to remove variable with near zero variability which contributes very little to the prediction. Apart from that, for the same reason I also remove username variable and timestamp variable from the training data. 


```{r}
##removing columns with near zero variablity variable
selCol <- nearZeroVar(training, saveMetrics=T)
selCol <- selCol[selCol$nzv==F,] 
selCol_training <- training[,row.names(selCol)]

##removing column with NA value greater than 95%
selCol_training <- selCol_training[, colMeans(is.na(selCol_training)) <= 0.95] 

##removing username and timestamp column
selCol_training <- selCol_training[,c(-1,-2,-3,-4,-5)]
```

In the end, I managed to reduce the variables from 160 to 54 variables.
```{r}
names(selCol_training)
length(names(selCol_training))
```

Training the data
------------------
Fitiing the filtered training data to the Caret Random forest model. 
```{r}
##NOTE: it takes approximately 3 hours to train the data using the random forest algorithm
ptm <- proc.time()
modFit <- train(classe~., data=selCol_training, method="rf", importance=T)
ptm <- proc.time()
ptm ## elapsed time is in seconds.
```

Out of sample error estimation  using the cross validation data
---------------------------------------------------------------

I am going to predict the outcome of the "classe" variable of the cross validation data. By doing so, I can figure out the accuracy of my machine learning model. 

From the lecture notes, out of sample error is defined as the error rate you get on a new data set. Sometimes called generalization error. The new data set would be the cross validation dataset which is 30% of the untrained data.
In short, the opposite of accuracy is out of sample error rate.

```{r}
accuracy <- confusionMatrix(crossVal$classe, predict(modFit, crossVal))
accuracy
```

The model managed to reach an accuracy of `r accuracy$overall['Accuracy']` when predicting the cross validation data. In other words the out of sample error rate is `r 1 - accuracy$overall['Accuracy']`.

Final Prediction using the provided test data
----------------------------------------------

```{r}
final_test_dat <- read.csv(file="pml-testing.csv")
answer = predict(modFit, final_test_dat)
answer
```
